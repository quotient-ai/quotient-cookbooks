{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Live Web Documentation QA Agent with Qdrant, Tavily, LangChain and Quotient\n",
    "\n",
    "This cookbook demonstrates building a system that automatically processes company documentation to power an intelligent question-answering agent. The system combines Tavily's intelligent web crawling, Qdrant's vector search, and Quotient's quality monitoring to create a robust documentation assistant.\n",
    "\n",
    "### Architecture Overview\n",
    "The system operates in two main phases. First, Tavily crawls your documentation website and extracts relevant content, which is then chunked, embedded, and stored in Qdrant for efficient similarity search. When users ask questions, the system retrieves the most relevant documentation snippets and generates contextual answers while Quotient monitors for hallucinations and retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use API keys from:\n",
    " - [OpenAI](www.openai.com) — get your API key from the [OpenAI API platform](https://platform.openai.com/login)\n",
    " - [Tavily](https://www.tavily.com/) — get your API key from the [Tavily app](https://app.tavily.com)\n",
    " - [Quotient AI](https://www.quotientai.co) — get your API key from the [Quotient AI app](https://app.quotientai.co)\n",
    " \n",
    "Both Tavily and Quotient offer generous free tiers to get started; you can check out their pricing  [here](https://www.tavily.com/#pricing) and [here](https://www.quotientai.co/pricing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set API keys:\n",
    "os.environ['QUOTIENT_API_KEY'] = \"quotient_api_key_here\"\n",
    "os.environ['OPENAI_API_KEY'] = \"openai_api_key_here\"\n",
    "os.environ['TAVILY_API_KEY'] = \"tavily_api_key_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU qdrant-client langchain langchain-openai langchain-qdrant sentence-transformers quotientai tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting up the core components\n",
    "\n",
    "The foundation uses OpenAI's `text-embedding-3-small` for embeddings and Open AI's `4o-mini` for generation, Qdrant as the vector database, and Quotient for comprehensive monitoring. The system initializes with hallucination detection and document relevancy scoring enabled to ensure reliable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<quotientai.client.QuotientLogger at 0x119e22a10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Initialize core components for the RAG system including models, vector store, and monitoring.\"\"\"\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from quotientai import QuotientAI, DetectionType\n",
    "from tavily import TavilyClient\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Initialize models\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "# Initialize Tavily client\n",
    "tavily_client = TavilyClient(api_key=os.environ['TAVILY_API_KEY'])\n",
    "\n",
    "# Initialize Quotient monitoring\n",
    "quotient = QuotientAI()\n",
    "quotient.logger.init(\n",
    "    app_name=\"qdrant-rag\",\n",
    "    environment=\"test\",\n",
    "    sample_rate=1.0,\n",
    "    detections=[DetectionType.HALLUCINATION, DetectionType.DOCUMENT_RELEVANCY],\n",
    "    detection_sample_rate=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the document processing pipeline\n",
    "\n",
    "The document processing transforms raw web content into searchable chunks. Tavily crawls documentation sites with specific instructions to focus on relevant pages, then the `RecursiveCharacterTextSplitter` creates overlapping 700-token chunks with 50-token overlap to preserve context at boundaries.\n",
    "\n",
    "The chunked documents are embedded and stored in Qdrant using the `QdrantVectorStore.from_documents()` method, which creates an in-memory collection for this demo but can easily be configured for persistent storage in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_and_process_docs(url, instructions):\n",
    "    \"\"\"Crawl documentation and convert to Document objects\"\"\"\n",
    "    response = tavily_client.crawl(url, instructions=instructions, timeout=120)\n",
    "    \n",
    "    documents = []\n",
    "    for result in response[\"results\"]:\n",
    "        # Create Document object with raw content\n",
    "        doc = Document(\n",
    "            page_content=result[\"raw_content\"],\n",
    "            metadata={\n",
    "                \"source\": result[\"url\"],\n",
    "                \"base_url\": response[\"base_url\"]\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the document processing pipeline\n",
    "def preprocess_dataset(docs_list):\n",
    "    \"\"\"Split documents into smaller chunks for better retrieval\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=700,\n",
    "        chunk_overlap=50,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "    return doc_splits\n",
    "\n",
    "# Create a retriever from documents using Qdrant vector store\n",
    "def create_retriever(collection_name, doc_splits):\n",
    "    \"\"\"Create a retriever from documents using Qdrant vector store\"\"\"\n",
    "    vectorstore = QdrantVectorStore.from_documents(\n",
    "        doc_splits,\n",
    "        OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "        location=\":memory:\",  # Use in-memory storage\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "    return vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement the RAG chain\n",
    "\n",
    "The RAG chain orchestrates the entire question-answering process with integrated monitoring. When a question comes in, the system retrieves relevant chunks using vector similarity, formats them as context, and generates an answer constrained to only use the provided documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the answer prompt template\n",
    "answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Answer the question using ONLY the provided documentation context. \n",
    "    If you don't know the answer, say \"I don't have enough information to answer that.\"\n",
    "    Include relevant quotes from the documentation to support your answer.\n",
    "    \n",
    "    IMPORTANT: Give ONLY your answer, do not include any system or human messages in your response.\"\"\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext: {context}\"),\n",
    "])\n",
    "\n",
    "# Function to format documents for context\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create a simple RAG chain\n",
    "def create_rag_chain(retriever):\n",
    "    \"\"\"Create a basic RAG chain with Quotient logging\"\"\"\n",
    "    \n",
    "    def get_context(question: str):\n",
    "        \"\"\"Get relevant documents for a question\"\"\"\n",
    "        docs = retriever.invoke(question)  # Using invoke instead of deprecated get_relevant_documents\n",
    "        return format_docs(docs), [doc.page_content for doc in docs]\n",
    "    \n",
    "    def generate_answer(question: str):\n",
    "        \"\"\"Generate an answer using RAG\"\"\"\n",
    "        # Get context from vector store\n",
    "        context, raw_docs = get_context(question)\n",
    "        \n",
    "        # Generate answer using LLM directly\n",
    "        prompt = f\"\"\"Answer the question using ONLY the provided documentation context.\n",
    "If you don't know the answer, say \"I don't have enough information to answer that.\"\n",
    "Include relevant quotes from the documentation to support your answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer: \"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt).content\n",
    "        \n",
    "        # Log to Quotient\n",
    "        quotient.log(\n",
    "            user_query=question,\n",
    "            model_output=response,\n",
    "            documents=raw_docs,\n",
    "            tags={\"timestamp\": datetime.now().isoformat()}\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    return generate_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crawling process targets specific documentation areas using targeted instructions. For the Qdrant example, the system focuses on Python client usage, vector search capabilities, filtering options, and getting started guides to ensure comprehensive coverage of the most commonly needed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling Qdrant documentation...\n",
      "Processing 40 documents...\n",
      "Added 1553 chunks to vector store\n"
     ]
    }
   ],
   "source": [
    "# Crawl Qdrant documentation\n",
    "print(\"Crawling Qdrant documentation...\")\n",
    "docs = crawl_and_process_docs(\n",
    "    \"https://qdrant.tech/documentation/\",\n",
    "    instructions=\"Find all pages about Python client usage, vector search, and getting started guides\"\n",
    ")\n",
    "\n",
    "# Split documents into chunks and create retriever\n",
    "print(f\"Processing {len(docs)} documents...\")\n",
    "doc_chunks = preprocess_dataset(docs)\n",
    "retriever = create_retriever(\"demo_docs\", doc_chunks)\n",
    "\n",
    "print(f\"Added {len(doc_chunks)} chunks to vector store\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Putting it all together: documentation QA agent with Quotient monitoring\n",
    "\n",
    "With all components in place, we can now run the agent with real questions about Qdrant's documentation. The system handles various query types and demonstrates both successful retrieval and graceful degradation when information isn't available.\n",
    "\n",
    "\n",
    "Every interaction is automatically logged to Quotient, which runs asynchronous detection pipelines to identify potential hallucinations and score document relevance. This monitoring helps maintain system reliability and provides insights into retrieval quality and answer accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG system with Qdrant documentation:\n",
      "\n",
      "Q: How do I get started with Qdrant's Python client?\n",
      "----------------------------------------\n",
      "To get started with Qdrant's Python client, you can follow the \"Quickstart\" guide available in the documentation. The guide covers the installation process and provides examples to help you begin using the client effectively.\n",
      "\n",
      "1. **Installation**: You can install the Qdrant client either with or without the `fastembed` option. The specific steps for installation are detailed in the Quickstart section.\n",
      "\n",
      "For more information, you can visit the Quickstart page directly: [Quickstart](https://python-client.qdrant.tech/quickstart).\n",
      "\n",
      "The documentation states, \"We’ll cover the following here: Installation with fastembed, Installation without fastembed.\" This indicates that the Quickstart guide will provide the necessary steps to set up the client.\n",
      "\n",
      "Q: What are the different filtering options in Qdrant?\n",
      "----------------------------------------\n",
      "The different filtering options in Qdrant include:\n",
      "\n",
      "1. **Filter.min_should**: This option allows specifying a minimum number of conditions that should be satisfied.\n",
      "2. **Filter.model_config**: This option is used to configure the model for filtering.\n",
      "3. **Filter.must**: This option specifies conditions that must be satisfied.\n",
      "4. **Filter.must_not**: This option specifies conditions that must not be satisfied.\n",
      "5. **Filter.should**: This option specifies conditions that should be satisfied.\n",
      "\n",
      "Additionally, there is a **FilterSelector** which includes:\n",
      "- **FilterSelector.filter**: This is used to apply a filter.\n",
      "- **FilterSelector.model_config**: Similar to the Filter, this configures the model for the selector.\n",
      "- **FilterSelector.shard_key**: This is used for specifying the shard key in the filtering process.\n",
      "\n",
      "These options provide flexibility in defining the criteria for filtering results in Qdrant.\n",
      "\n",
      "Q: How does vector search work in Qdrant and what distance metrics are supported?\n",
      "----------------------------------------\n",
      "Vector search in Qdrant works by utilizing a graph-like structure, specifically the HNSW (Hierarchical Navigable Small World) algorithm, to efficiently find the closest objects in sublinear time. This approach avoids the need to calculate the distance to every object in the database, instead focusing on a subset of candidates. As stated in the documentation, \"Qdrant is a fully-fledged vector database that speeds up the search process by using a graph-like structure to find the closest objects in sublinear time.\"\n",
      "\n",
      "Regarding distance metrics, the documentation does not provide specific details on the supported distance metrics for vector search in Qdrant. Therefore, I don't have enough information to answer that.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system with Qdrant documentation\n",
    "answer_generator = create_rag_chain(retriever)\n",
    "\n",
    "questions = [\n",
    "    \"How do I get started with Qdrant's Python client?\",\n",
    "    \"What are the different filtering options in Qdrant?\",\n",
    "    \"How does vector search work in Qdrant and what distance metrics are supported?\"\n",
    "]\n",
    "\n",
    "print(\"Testing RAG system with Qdrant documentation:\\n\")\n",
    "for question in questions:\n",
    "    print(f\"Q: {question}\")\n",
    "    print(\"-\" * 40)\n",
    "    response = answer_generator(question)\n",
    "    print(f\"{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Review detections in Quotient\n",
    "\n",
    "You can now view your logs and detections in the [Quotient dashboard](app.quotientai.co), where you can also filter them by tags and environments to identify common failure patterns.\n",
    "\n",
    "![Quotient AI Dashboard](Quotient_Dashboard.png \"Quotient AI Dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You've Built\n",
    "A documentation QA system that:\n",
    "\n",
    "- Crawls and processes live web documentation using Tavily\n",
    "- Stores documents in Qdrant for efficient retrieval\n",
    "- Generates contextual answers using retrieved documentation chunks\n",
    "- Monitors answer quality with Quotient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to interpret the results\n",
    "\n",
    "- Well-grounded systems typically show **< 5% hallucination rate**. If yours is higher, it's often a signal that either your data ingestion, retrieval pipeline, or prompting needs improvement.\n",
    "- High-performing systems typically show **> 75% document relevance**. Lower scores may signal ambiguous user queries, incorrect retrieval, or noisy source data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tavily-quotient-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
