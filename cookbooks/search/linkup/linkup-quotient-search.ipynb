{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Building a Real-Time Search Agent with Linkup & Quotient\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/quotient-ai/quotient-cookbooks/blob/main/cookbooks/search/linkup/linkup-quotient-detections.ipynb\">\n",
        " <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "This cookbook demonstrates how to build a real-time search agent using [Linkup](https://docs.linkup.so/) while monitoring result accuracy with [Quotient AI](https://www.quotientai.co/).\n",
        "\n",
        "We'll cover:\n",
        "- Using Linkup's API to perform real-time web search\n",
        "- Processing search queries from a JSONL file\n",
        "- Monitoring search result quality and detecting hallucinations with Quotient\n",
        "- Understanding and improving the accuracy of search results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "! pip install -qU quotientai linkup-sdk tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 0: Grab your API keys\n",
        "\n",
        "We'll use API keys from:\n",
        " - [Linkup](https://docs.linkup.so/) ‚Äî get your API key from the [Linkup app](https://app.linkup.so/sign-up)\n",
        " - [Quotient AI](https://www.quotientai.co) ‚Äî get your API key from the [Quotient AI app](https://app.quotientai.co)\n",
        " \n",
        "Both Linkup and Quotient offer generous free tiers to get started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# Set API keys:\n",
        "os.environ['LINKUP_API_KEY'] = \"your-linkup-api-key\"\n",
        "os.environ['QUOTIENT_API_KEY'] = \"your-quotient-api-key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Set up Linkup Search and Quotient Monitoring\n",
        "\n",
        "We'll configure Linkup's API to perform real-time web search and use Quotient to monitor the quality of our search results. Our search queries will cover various topics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from linkup import LinkupClient\n",
        "from typing import Dict, Any\n",
        "import json\n",
        "\n",
        "# Initialize the client\n",
        "client = LinkupClient(api_key=os.getenv('LINKUP_API_KEY'))\n",
        "\n",
        "def get_search_results(query: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Get search results for a query using Linkup's sourcedAnswer.\n",
        "\n",
        "    Args:\n",
        "        query: The search query\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing the answer and source documents\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Clean input\n",
        "        query = query.strip()\n",
        "        \n",
        "        # Call Linkup API\n",
        "        response = client.search(\n",
        "            query=query,\n",
        "            depth=\"deep\",  # Use deep for more thorough results\n",
        "            output_type=\"sourcedAnswer\"\n",
        "        )\n",
        "        \n",
        "        # Access attributes directly from the LinkupSourcedAnswer object\n",
        "        return {\n",
        "            'answer': response.answer,\n",
        "            'sources': response.sources\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'error': str(e),\n",
        "            'query': query\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "Quotient is an intelligent observability platform designed for retrieval-augmented and search-augmented AI systems.\n",
        "\n",
        "Quotient performs automated detections on two key fronts each time you send it a log:\n",
        "\n",
        "- **Hallucination:** Identifies statements in the model output that are unsupported by the retrieved documents or that contradict them. This flagging is done at the sentence level and returns a boolean indicator if any part of the answer contains a hallucination.\n",
        "\n",
        "- **Document Relevance:** Evaluates each retrieved document to determine whether it meaningfully contributed to grounding the answer. Quotient returns relevance labels for all documents, helping gauge retrieval and search quality.\n",
        "  \n",
        "These capabilities are enabled automatically when `hallucination_detection=True` is set during logger initialization.\n",
        "\n",
        "Below, we'll set up the Quotient logger, send each AI-search result for automatic evaluation, and retrieve structured logs and detections:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from quotientai import QuotientAI, DetectionType\n",
        "\n",
        "# Initialize the Quotient SDK\n",
        "quotient = QuotientAI()\n",
        "\n",
        "logger = quotient.logger.init(\n",
        "    # Name your application or project\n",
        "    app_name=\"linkup-search-agent-demo\",\n",
        "    # Set the environment (e.g., \"dev\", \"prod\", \"staging\")\n",
        "    environment=\"test\",\n",
        "    # Set the sample rate for logging (0-1.0)\n",
        "    sample_rate=1.0,\n",
        "    # this will automatically run hallucination detection on 100% of your model outputs in relation to the documents you provide\n",
        "    detections=[DetectionType.HALLUCINATION, DetectionType.DOCUMENT_RELEVANCY],\n",
        "    detection_sample_rate=1.0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Load and Process Search Queries\n",
        "\n",
        "We'll load search queries from a JSONL file that contains various real-world questions. Each query will be processed to:\n",
        "1. Get relevant search results from Linkup\n",
        "2. Monitor result quality with Quotient\n",
        "3. Track performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 140 queries. First 3 queries:\n",
            "1. What‚Äôs the difference between GPT-4o and Claude 3?\n",
            "2. Who is the CEO of Anthropic?\n",
            "3. Summarize today‚Äôs top stories on climate change.\n"
          ]
        }
      ],
      "source": [
        "def load_queries(file_path: str, max_queries: int = 100) -> list:\n",
        "    \"\"\"\n",
        "    Load queries from a JSONL file.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to the JSONL file\n",
        "        max_queries: Maximum number of queries to load\n",
        "        \n",
        "    Returns:\n",
        "        List of query strings\n",
        "    \"\"\"\n",
        "    queries = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_queries:\n",
        "                break\n",
        "            query_obj = json.loads(line)\n",
        "            queries.append(query_obj['query'])\n",
        "    return queries\n",
        "\n",
        "# Load example queries\n",
        "queries = load_queries('example_queries.jsonl', max_queries=120)\n",
        "print(f\"Loaded {len(queries)} queries. First 3 queries:\")\n",
        "for i, query in enumerate(queries[:3]):\n",
        "    print(f\"{i+1}. {query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "Let's generate detailed, structured descriptions for each company using Linkup's API. For each company, we will:\n",
        "1. Generate a comprehensive search query\n",
        "2. Use Linkup's structured output feature to extract company information\n",
        "3. Format the results according to our schema\n",
        "4. Log the results in Quotient for quality monitoring\n",
        "\n",
        "The structured output ensures consistent formatting and makes it easy to use the company information in downstream applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing query 1/140: What do people say about the Rabbit R1 AI device?\n",
            "\n",
            "üìä Search Results:\n",
            "People's opinions about the Rabbit R1 AI device are mixed but generally critical:\n",
            "\n",
            "- Many reviewers find the device underwhelming and \"half-baked,\" with features that often don't work as promised or are limited in functionality. (WIRED, Gizmodo, The Shortcut)\n",
            "- The scroll wheel and physical interface receive criticism for being inconsistent and frustrating to use. (WIRED)\n",
            "- Users question the need for a separate device when smartphones can perform similar tasks, making the R1 feel redundant and inconvenient to carry. (Reddit r/ArtificialIntelligence, r/hardware)\n",
            "- Battery life and software bugs were issues at launch, though some improvements have been made via updates. (The Shortcut)\n",
            "- The device is praised for its design and concept, with some seeing it as a glimpse into the future of AI interaction, but many feel it is not ready for mainstream use. (Pixel Refresh, Reddit r/Rabbitr1)\n",
            "- The Large Action Model (LAM) feature, which allows the device to learn and automate web tasks, is promising but still in early stages and not fully realized. (ZDNET)\n",
            "- Some users appreciate the device as a fun experimental gadget and are hopeful for future improvements, while others consider it a poor value for the price. (Reddit r/Rabbitr1, Tom's Guide)\n",
            "- Comparisons to similar devices like the Humane AI Pin often highlight that the R1 is somewhat better but still lacks compelling advantages over existing smartphone apps. (Wikipedia, Reddit r/LocalLLaMA)\n",
            "\n",
            "In summary, the Rabbit R1 is seen as an interesting but flawed early attempt at a dedicated AI assistant device, with many users and reviewers advising caution before purchase due to its current limitations and overlap with smartphone capabilities.\n",
            "üìù Logged to Quotient with log_id: c56658be-5a95-4342-80ea-1dae82a6a4f9\n",
            "\n",
            "Processing query 2/140: What is the latest climate tech receiving VC funding?\n",
            "\n",
            "üìä Search Results:\n",
            "The latest climate tech receiving VC funding in 2025 focuses on several key areas:\n",
            "\n",
            "- Energy sector innovations, especially clean power, energy storage, nuclear, and geothermal technologies, are receiving the largest amounts of funding.\n",
            "- Carbon and emissions technology startups set a new record with $7.6 billion in VC funding in Q3 2025, including strong deals in manufacturing & chemicals, lithium battery recycling, and green mining.\n",
            "- AI-enabled climate solutions are increasingly competitive, attracting about 20% of climate tech investment dollars, particularly in North America.\n",
            "- Emerging tech areas like hydrogen and carbon capture, utilization, and storage (CCUS) are seeing increased strategic investor participation.\n",
            "- Hardware and grid technologies are gaining renewed interest.\n",
            "- Climate adaptation technologies are catching up to mitigation in funding focus.\n",
            "- Early-stage climate tech startups continue to attract VC interest despite overall funding declines, with investors emphasizing scalable, impact-driven solutions.\n",
            "\n",
            "Top VC funds active in 2025 include World Fund, Climate Capital, Norrsken VC, Contrarian Ventures, and others focusing on startups with significant emissions reduction potential across energy, food and agriculture, manufacturing, buildings, and mobility sectors.\n",
            "\n",
            "In summary, the latest VC funding in climate tech in 2025 is concentrated on advanced energy technologies, carbon and emissions reduction innovations, AI-driven solutions, hydrogen and CCUS, and hardware/grid tech, with strong support from specialized climate-focused VC funds.\n",
            "üìù Logged to Quotient with log_id: 12cdae01-7f19-459f-8f25-8c1a6027492b\n",
            "\n",
            "Processing query 3/140: How are autonomous agents being evaluated in the wild?\n",
            "\n",
            "üìä Search Results:\n",
            "Autonomous agents are evaluated in the wild using a combination of automated, scalable, and realistic methods that aim to reflect real-world conditions and challenges. Key approaches include:\n",
            "\n",
            "1. **Automated Evaluation Models**:  \n",
            "   - Vision-language models (VLMs) and language models (LMs) are used to autonomously evaluate agent trajectories and performance by reasoning over textual and visual data (e.g., screenshots).  \n",
            "   - Two main variants are used: a modular caption-then-reason approach and an end-to-end approach with advanced VLMs like GPT-4V. These methods achieve high agreement (74.4% to 92.9%) with oracle evaluation metrics on benchmarks such as WebArena and Android-in-the-Wild (AitW).  \n",
            "   - These evaluators can also serve as discriminators to refine agent policies autonomously through techniques like inference-time policy refinement and filtered behavior cloning.\n",
            "\n",
            "2. **Real-World and Large-Scale Datasets**:  \n",
            "   - Agents are tested on large, diverse datasets such as Android-in-the-Wild, which contains hundreds of thousands of human demonstrations and thousands of unique instructions, enabling evaluation in realistic device control scenarios.\n",
            "\n",
            "3. **Autonomous Reinforcement Learning (RL) in Real Environments**:  \n",
            "   - Methods like DigiRL train agents directly in-the-wild using autonomous RL, with online and offline phases, improving success rates significantly over prior methods.  \n",
            "   - These agents are evaluated on real device emulators and physical devices, demonstrating improved performance in diverse tasks.\n",
            "\n",
            "4. **Safety and Risk Monitoring**:  \n",
            "   - Safety monitors (e.g., AgentMonitor) are designed to detect and stop unsafe behaviors during real-world testing, addressing risks of harm and malicious interactions.\n",
            "\n",
            "5. **Simulation and Large-Scale Testing Platforms**:  \n",
            "   - Companies like Coval use large-scale simulation platforms to identify issues and improve agent reliability before deployment, drawing parallels to testing in autonomous vehicles.\n",
            "\n",
            "6. **Continuous and Integrated Evaluation**:  \n",
            "   - Evaluation is integrated throughout the agent lifecycle, from development to deployment, including unit, integration, and end-to-end testing, as well as ongoing monitoring in live environments to ensure robustness and alignment.\n",
            "\n",
            "7. **Code-Based and Autonomous Frameworks**:  \n",
            "   - Some approaches assess agent autonomy through code inspection to reduce risks and costs of runtime evaluation.  \n",
            "   - Frameworks like AutoEval enable autonomous evaluation of mobile agents by automatically generating task reward signals and using judge systems to score performance without manual effort.\n",
            "\n",
            "8. **Real-World Interaction and Adaptation**:  \n",
            "   - Evaluations consider agents' ability to autonomously acquire resources, replicate, and adapt to novel challenges encountered in the wild, reflecting their real-world operational capabilities.\n",
            "\n",
            "In summary, autonomous agents in the wild are evaluated through a mix of automated model-based assessments, large-scale real-world datasets, autonomous reinforcement learning in realistic environments, safety monitoring, simulation platforms, continuous lifecycle testing, and autonomous evaluation frameworks. These methods aim to ensure agents perform reliably, safely, and adaptively in complex, dynamic real-world settings.\n",
            "üìù Logged to Quotient with log_id: 906a116f-5ea6-47e1-9d3b-1816fc5ea4b4\n",
            "\n",
            "Processing query 4/140: What‚Äôs new in the Hugging Face Transformers library?\n",
            "\n",
            "üìä Search Results:\n",
            "Recent updates and new features in the Hugging Face Transformers library include:\n",
            "\n",
            "- Addition of new models: Ernie 4.5 and its MoE variant (Ernie 4.5 MoE), and ModernBERT Decoder, available from specific preview release tags (v4.53.2-Ernie-4.5-preview and v4.53.2-modernbert-decoder-preview).\n",
            "- Introduction of the Encoder-only Mask Transformer (EoMT) model, highlighted in the CVPR 2025 paper.\n",
            "- Continued role as a central model-definition framework compatible across many training frameworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, etc.) and inference engines (vLLM, SGLang, TGI, etc.).\n",
            "- Integration with vLLM for high-throughput, production-ready inference, leveraging transformers backend for flexibility and efficiency.\n",
            "- Support for a vast ecosystem of state-of-the-art models across text, vision, audio, video, and multimodal tasks, with over 1 million model checkpoints on the Hugging Face Hub.\n",
            "- Ongoing improvements to simplify, customize, and optimize model definitions for both training and inference.\n",
            "- Availability of comprehensive courses and documentation to learn transformer models from fundamentals to advanced applications.\n",
            "\n",
            "These updates reflect Hugging Face‚Äôs commitment to democratizing AI by providing a unified, extensible, and efficient platform for state-of-the-art transformer models.\n",
            "üìù Logged to Quotient with log_id: ffd2f336-fc45-4f3e-80e3-5a2142b27f0b\n",
            "\n",
            "Processing query 5/140: What are Redditors saying about the Humane AI Pin?\n",
            "\n",
            "üìä Search Results:\n",
            "Redditors generally express strong disappointment and criticism about the Humane AI Pin. Common sentiments include:\n",
            "\n",
            "- The product is seen as overhyped but underdelivering, with many calling it gimmicky and not solving problems that smartphones already handle.\n",
            "- Users report poor performance, slow processing due to reliance on cloud servers, and limited functionality.\n",
            "- The device is expensive ($700 plus a $24 monthly subscription) and many feel it is not worth the cost.\n",
            "- There is frustration over Humane shutting down the AI Pin services, rendering the device mostly useless and without refunds.\n",
            "- Some users criticize Humane as a poorly run company that marketed the product with \"half-truths\" and failed to meet expectations.\n",
            "- Comparisons to other devices like Apple Watch or upcoming AI assistants suggest the AI Pin is already obsolete or inferior.\n",
            "- There is some discussion about jailbreaking or repurposing the device, but Humane‚Äôs sale of IP to HP complicates this.\n",
            "- A few acknowledge it as a first-generation product with potential but agree it is not ready for mainstream use.\n",
            "\n",
            "Overall, the Reddit community views the Humane AI Pin as a failed, overpriced, and prematurely discontinued product with limited practical value.\n",
            "üìù Logged to Quotient with log_id: eae386a2-1e6f-45b9-bf9d-e5463a5d5535\n",
            "\n",
            "Processing query 6/140: What are investors saying about AI chipmakers in 2025?\n",
            "\n",
            "üìä Search Results:\n",
            "Investors in 2025 are highly optimistic about AI chipmakers, particularly NVIDIA, AMD, and Qualcomm, due to strong growth prospects driven by rising AI infrastructure spending and demand for AI-enabled devices. NVIDIA leads the market with innovations like the Blackwell architecture and new products such as the Dynamo inference framework and Project Digits AI supercomputer, fueling significant revenue growth (69% jump to $44.1 billion) despite regulatory challenges. Microsoft‚Äôs $80 billion investment in AI data centers and increased spending by other tech giants (Alphabet, Amazon, Meta) bolster demand for AI chips, creating a multibillion-dollar opportunity. Qualcomm is well-positioned in the generative AI smartphone chipset market, and AMD is expanding through acquisitions and competitive valuations. The semiconductor market is expected to grow strongly in 2025, with AI chip revenues projected to reach nearly $92 billion, attracting investor interest in both established players and innovative startups. Overall, investors view AI chipmakers as key beneficiaries of the AI boom, with expectations of continued double-digit growth and strong market demand.\n",
            "üìù Logged to Quotient with log_id: d580f59a-a6a4-48c5-9e79-8562a092649a\n",
            "\n",
            "Processing query 7/140: What‚Äôs the verdict on Google‚Äôs Gemini Ultra performance?\n",
            "\n",
            "üìä Search Results:\n",
            "Google's Gemini Ultra is widely regarded as a highly advanced and powerful AI model that outperforms GPT-4 on many benchmarks. It excels in complex reasoning, multimodal tasks (text, images, audio, video), coding, and research. Gemini Ultra achieves state-of-the-art results on 30 of 32 major academic benchmarks, including surpassing human expert performance on the MMLU test. It offers deep integration with Google Workspace and Google Drive, with premium plans providing up to 30TB storage and exclusive features like task-streamlining agents and advanced video generation.\n",
            "\n",
            "However, some reviews note areas for improvement, such as challenges in interpreting certain visual data. Overall, Gemini Ultra is seen as Google's flagship AI, delivering faster, more detailed responses and exceptional scalability, making it suitable for enterprise and complex applications. It is considered a significant step forward in AI performance, though not a revolutionary leap beyond existing top models.\n",
            "\n",
            "In summary, Gemini Ultra's verdict is that it is a top-tier, highly capable AI model with strong multimodal and reasoning abilities, superior benchmark performance, and deep Google ecosystem integration, but with some room to improve in specific visual interpretation tasks.\n",
            "üìù Logged to Quotient with log_id: b3365ec0-158b-46ce-b276-770718381597\n",
            "\n",
            "Processing query 8/140: What topics dominated OpenAI DevDay 2025?\n",
            "\n",
            "üìä Search Results:\n",
            "The dominant topics at OpenAI DevDay 2025 included:\n",
            "\n",
            "- Updates and previews related to GPT-5, including insights into its development and testing.\n",
            "- Introduction of new research-grade models (o3-deep-research and o4-mini-deep-research) for specialized roles.\n",
            "- Expansion of ChatGPT features such as Personality Modes and the new \"Study Together\" mode designed for guided learning and exam preparation.\n",
            "- Developments in generative video AI, notably the upcoming Sora 2 model.\n",
            "- Hands-on workshops and demos showcasing the latest OpenAI models and tools.\n",
            "- Discussions on platform direction, model deployment challenges, and capacity issues.\n",
            "\n",
            "Overall, the event focused on advancing AI capabilities, improving educational tools, and preparing for next-generation AI models and applications.\n",
            "üìù Logged to Quotient with log_id: f9cc802f-8e7d-48b4-afe2-aa1507e3ff6e\n",
            "\n",
            "Processing query 9/140: What‚Äôs the impact of multimodal models on accessibility?\n",
            "\n",
            "üìä Search Results:\n",
            "Multimodal models significantly enhance accessibility by providing multiple ways for users to interact with technology, accommodating diverse abilities and preferences. They enable systems to process and integrate various input types (e.g., text, images, audio, gestures), which helps overcome personal limitations and sensory impairments. For example, multimodal AI can generate descriptive captions for images, aiding visually impaired users, and support multimodal conversational AI that allows communication through voice, text, and visual cues. Multimodal interfaces increase usability for people of different ages, skill levels, and disabilities by offering flexible interaction choices and reducing accessibility barriers. This leads to more inclusive, natural, and engaging user experiences across digital products and services.\n",
            "üìù Logged to Quotient with log_id: 55928385-ac91-435b-8650-a3fc689ba837\n",
            "\n",
            "Processing query 10/140: What are the latest LLM benchmarks published on Papers with Code?\n",
            "\n",
            "üìä Search Results:\n",
            "The latest LLM benchmarks published on Papers with Code include:\n",
            "\n",
            "1. **MT-Bench and Chatbot Arena**: Multi-turn question set and crowdsourced battle platform for evaluating LLM-based chat assistants, focusing on human preference alignment and reasoning abilities.\n",
            "\n",
            "2. **LLM Health Benchmarks Dataset**: Specialized for evaluating LLMs in medical specialties with structured question-answer pairs.\n",
            "\n",
            "3. **MMLU (Massive Multitask Language Understanding)**: Covers 57 subjects across STEM, humanities, social sciences, testing zero-shot and few-shot knowledge and problem-solving.\n",
            "\n",
            "4. **Arena-Hard Dataset**: A challenging benchmark with 500 user queries from live Chatbot Arena data, judged by GPT-4-Turbo, designed to robustly separate model capabilities.\n",
            "\n",
            "5. **LLM-SRBench**: Scientific equation discovery benchmark with 239 problems across four scientific domains, testing reasoning beyond memorization.\n",
            "\n",
            "6. **Web-Bench**: Code benchmark based on web standards and frameworks, evaluating LLMs' coding capabilities in web development contexts.\n",
            "\n",
            "7. **AgentBench Dataset**: Evaluates LLMs as agents in interactive environments, including SQL-based and game-based tasks.\n",
            "\n",
            "8. **Multiple Choice Question Answering (MCQA)**: Benchmarks focused on reasoning and analogy-making in multiple-choice QA settings.\n",
            "\n",
            "These represent some of the newest and diverse benchmarks targeting different LLM capabilities such as chat interaction, domain-specific knowledge, multitask understanding, scientific reasoning, code generation, and agent behavior.\n",
            "üìù Logged to Quotient with log_id: e025f50d-00b2-4b37-bdc9-25bb2ff58d69\n",
            "\n",
            "Processing query 11/140: What‚Äôs the public reception of the Humane AI Pin on YouTube?\n",
            "\n",
            "üìä Search Results:\n",
            "The public reception of the Humane AI Pin on YouTube is largely negative. Influential tech reviewer Marques Brownlee (MKBHD) called it \"The Worst Product I've Ever Reviewed,\" highlighting its poor performance and frustrating user experience. Other reviewers describe the device as slow, finicky, and not smart enough, with issues like overheating and a high price ($699 plus subscription). The AI Pin is criticized for failing to replace smartphones effectively and for its awkward design and limited functionality. Despite some appreciation for its innovative concept, the consensus is that the execution is flawed and the product does not meet expectations.\n",
            "üìù Logged to Quotient with log_id: 4451cdee-bc53-4efc-939a-713f5aaee75e\n",
            "\n",
            "Processing query 12/140: What is LangChain used for in AI agent development?\n",
            "\n",
            "üìä Search Results:\n",
            "LangChain is used in AI agent development as a powerful open-source framework that enables developers to build intelligent AI agents that use large language models (LLMs) as reasoning engines. It helps these agents decide which actions to take and which tools to use by integrating LLMs with external APIs, data sources, and tools. LangChain supports multi-step workflows, conversational memory, and tool invocation, allowing agents to perform complex tasks autonomously and interact dynamically with the outside world. It provides abstractions and components to simplify orchestrating tasks, managing memory, and building stateful, scalable, and customizable AI agents from prototype to production.\n",
            "üìù Logged to Quotient with log_id: f49f6654-1bd4-44da-8dcc-d1edc5cf40bb\n",
            "\n",
            "Processing query 13/140: Summarize the key takeaways from OpenAI‚Äôs recent blog posts.\n",
            "\n",
            "üìä Search Results:\n",
            "Key takeaways from OpenAI‚Äôs recent blog posts include:\n",
            "\n",
            "1. Release of GPT-4.5 research preview, advancing pre-training and post-training scaling.\n",
            "2. Launch of GPT-4 Turbo with 128K context length, vision capabilities, improved instruction following, JSON mode, and lower prices.\n",
            "3. Introduction of new Assistants API and DALL¬∑E 3 API for image generation integrated into apps.\n",
            "4. Function calling improvements allowing multiple function calls in one message and better accuracy.\n",
            "5. New features like reproducible outputs via a seed parameter and upcoming log probabilities for tokens.\n",
            "6. Commitment to user privacy and legal defense through Copyright Shield for copyright claims.\n",
            "7. Open sourcing of Whisper large-v3 speech recognition model and Consistency Decoder for image enhancement.\n",
            "8. ChatGPT agent now can proactively perform complex tasks using browsing, code execution, and analysis.\n",
            "9. Leadership transition with Mira Murati appointed interim CEO.\n",
            "10. Structural evolution of OpenAI to a Public Benefit Corporation to reinforce mission alignment.\n",
            "\n",
            "These updates reflect OpenAI‚Äôs focus on improving model capabilities, developer tools, user safety, and broadening AI accessibility and impact.\n",
            "üìù Logged to Quotient with log_id: dd131748-3f35-4c6b-a026-fe365dfa74fa\n",
            "\n",
            "Processing query 14/140: What are the newest features in the ChatGPT desktop app?\n",
            "\n",
            "üìä Search Results:\n",
            "The newest features in the ChatGPT desktop app include:\n",
            "\n",
            "- Integration of the GPT-4o model, which is more intuitive, creative, and collaborative, with improved writing capabilities that are natural, audience-aware, and tailored for relevance and readability.\n",
            "- Advanced Voice Mode allowing real-time, natural voice conversations with ChatGPT on desktop, enabling hands-free advice and interaction.\n",
            "- Ability to take and discuss screenshots directly within the app.\n",
            "- File upload support for generating summaries, identifying trends, and extracting insights.\n",
            "- Instant access via keyboard shortcuts (Alt + Space on Windows, Option + Space on macOS).\n",
            "- Native macOS app features like voice interaction and trackpad haptics.\n",
            "- Integration with desktop applications and developer tools (e.g., VS Code, Xcode, Terminal) on macOS.\n",
            "- Multimodal capabilities including text, image, and audio processing.\n",
            "- Conversations and files are preserved during active sessions with some retention after pausing.\n",
            "- The app runs more efficiently than the browser version, especially for tasks using tools like Python, DALL-E, or memory features.\n",
            "\n",
            "These features make the desktop app a more seamless, productive, and interactive AI assistant experience on both macOS and Windows.\n",
            "üìù Logged to Quotient with log_id: 7144c6af-2680-441c-8a2c-173eea70481d\n",
            "\n",
            "Processing query 15/140: What‚Äôs trending in r/LanguageTechnology on Reddit?\n",
            "\n",
            "üìä Search Results:\n",
            "No data is available about the current trending topics in r/LanguageTechnology on Reddit.\n",
            "üìù Logged to Quotient with log_id: fbb97b22-846d-4d8b-b637-233fd12a450a\n",
            "\n",
            "Processing query 16/140: Who are the top-performing AI chip manufacturers in 2025?\n",
            "\n",
            "üìä Search Results:\n",
            "The top-performing AI chip manufacturers in 2025 are:\n",
            "\n",
            "1. **NVIDIA** ‚Äì The dominant leader in AI chips, especially GPUs for AI workloads, with innovations like the Blackwell platform offering massive performance and energy efficiency improvements.\n",
            "\n",
            "2. **AMD** ‚Äì A strong competitor with high-performance AI chips such as the MI350 series and Ryzen AI Pro processors, enhanced by acquisitions to boost AI hardware and software capabilities.\n",
            "\n",
            "3. **TSMC** ‚Äì The world‚Äôs leading semiconductor foundry, manufacturing advanced AI chips for many top designers including NVIDIA and AMD, with cutting-edge fabrication technologies (3nm and 2nm processes).\n",
            "\n",
            "4. **Apple** ‚Äì Known for its custom AI-focused Neural Engine chips integrated into its M-series processors (M4 and upcoming M5), delivering high AI performance and efficiency for its devices.\n",
            "\n",
            "5. **Intel** ‚Äì Expanding its AI chip portfolio with new products like Gaudi 3 and Falcon Shores GPUs, focusing on HPC and AI-optimized compute, and collaborating with Microsoft.\n",
            "\n",
            "6. **Google (Alphabet)** ‚Äì Developer of Tensor Processing Units (TPUs), with the latest TPU v6e and upcoming TPU v7 offering significant performance gains for AI workloads.\n",
            "\n",
            "Other notable companies making significant strides include Samsung, AWS, Cerebras, Qualcomm, Broadcom, and innovative startups like Tenstorrent, Groq, Mythic, and Blumind.\n",
            "\n",
            "These companies lead the AI chip market through technological innovation, commercial impact, and addressing diverse AI applications from data centers to edge devices.\n",
            "üìù Logged to Quotient with log_id: f4fecdb1-b78b-48b4-b228-0588bc8ac901\n",
            "\n",
            "Processing query 17/140: What‚Äôs the market share breakdown between AWS, Azure, and GCP in 2025?\n",
            "\n",
            "üìä Search Results:\n",
            "In 2025, the market share breakdown for the top three cloud providers is approximately:\n",
            "\n",
            "- AWS: 29% to 32%  \n",
            "- Microsoft Azure: 20% to 25%  \n",
            "- Google Cloud Platform (GCP): 7% to 12%\n",
            "\n",
            "Specifically, Q1 2025 data from Synergy Research Group shows AWS leading with about 29% market share, Microsoft Azure around 24%, and Google Cloud about 7%. Other sources indicate AWS holds roughly 30-32%, Azure about 20-25%, and GCP between 11-12%. Combined, these three control about 63-67% of the global cloud infrastructure market in 2025.\n",
            "üìù Logged to Quotient with log_id: 57b0734e-b007-46e9-b398-15e45e6acd14\n",
            "\n",
            "Processing query 18/140: Find articles comparing Claude 3 and GPT-4o on academic tasks.\n",
            "\n",
            "üìä Search Results:\n",
            "Several articles compare Claude 3 (including Claude 3.5 Sonnet and Claude 3 Opus) and GPT-4o on academic tasks:\n",
            "\n",
            "1. **Claude 3.5 vs. GPT-4o on academic and reasoning tasks**  \n",
            "   - Claude 3.5 Sonnet excels in complex academic reasoning, nuanced understanding, and detailed responses, often preferred for graduate-level reasoning, math problem-solving (e.g., GSM-8k), and legal contract analysis.  \n",
            "   - GPT-4o shows strengths in mathematical reasoning benchmarks (e.g., scoring higher on MATH benchmark), speed, and multimodal capabilities but is sometimes seen as more generic in creative or nuanced writing.  \n",
            "   - Claude 3.5 is noted for producing more thoughtful, multi-perspective answers valuable for research and strategic planning.\n",
            "\n",
            "2. **Coding and programming tasks**  \n",
            "   - Claude 3.5 Sonnet and Claude 3 Opus outperform GPT-4o in coding benchmarks (HumanEval), generating more accurate and bug-free code on first attempts.  \n",
            "   - GPT-4o performs well in standardized coding problems but is generally rated slightly behind Claude models in coding quality.\n",
            "\n",
            "3. **Natural language understanding and writing**  \n",
            "   - Claude models are praised for more natural, human-like, and creative writing, with better style consistency and ethical safety measures.  \n",
            "   - GPT-4o is faster and more versatile for general text generation, including multimodal inputs (text, images, audio).\n",
            "\n",
            "4. **Benchmark and user feedback summaries**  \n",
            "   - Claude 3 Opus and Claude 3.5 Sonnet generally outperform GPT-4o in complex reasoning, academic benchmarks, and long-context tasks.  \n",
            "   - GPT-4o has a slight edge in speed and some mathematical reasoning tests but may lack the depth and nuance Claude provides in academic and research contexts.  \n",
            "   - User reviews highlight Claude as better for detailed, context-rich academic tasks, while GPT-4o is favored for quick, broad applications.\n",
            "\n",
            "In summary, for academic tasks requiring deep reasoning, nuanced understanding, and complex problem-solving, Claude 3.5 (especially Sonnet) is often considered superior to GPT-4o. GPT-4o excels in speed, multimodal tasks, and some mathematical reasoning but is generally seen as less specialized for in-depth academic work.\n",
            "üìù Logged to Quotient with log_id: 2d5e2d44-7784-4557-b04e-61715d5d9763\n",
            "\n",
            "Processing query 19/140: What does the latest Stack Overflow developer survey say about LLM usage?\n",
            "\n",
            "üìä Search Results:\n",
            "The latest 2025 Stack Overflow Developer Survey reveals the following about LLM (Large Language Model) usage:\n",
            "\n",
            "- 84% of developers use or plan to use AI tools in their development process, up from 76% in 2024.\n",
            "- The most used LLM models are OpenAI‚Äôs GPT models (81%), Claude Sonnet models (43%), and Gemini Flash models (35%).\n",
            "- Despite increased usage, trust in AI outputs is low, with 46% of developers not trusting AI-generated code accuracy, up from 31% last year.\n",
            "- \"Vibe coding\" (generating software from LLM prompts) is not widely adopted professionally; nearly 77% of developers said it is not part of their work.\n",
            "- AI agents are used by only 31% of developers currently, with 17% planning to use them.\n",
            "- Developers still prefer human interaction and knowledge exchange, relying heavily on platforms like Stack Overflow (84%), GitHub (67%), and YouTube (61%).\n",
            "- Debugging AI-generated code is a major frustration (45% of respondents).\n",
            "- Usage of AI-enabled IDEs is growing, with Cursor (18%), Claude Code (10%), and Windsurf (5%) gaining traction.\n",
            "\n",
            "In summary, LLMs and AI tools are increasingly integrated into developer workflows, but trust issues and limited professional adoption of full AI-generated coding remain significant.\n",
            "üìù Logged to Quotient with log_id: 632cce45-74dd-4938-ab21-794ae7633653\n"
          ]
        }
      ],
      "source": [
        "log_ids = []\n",
        "\n",
        "for i, query in enumerate(queries, 1):\n",
        "    print(f\"\\nProcessing query {i}/{len(queries)}: {query}\")\n",
        "    result = get_search_results(query)\n",
        "    \n",
        "    if 'error' in result:\n",
        "        print(f\"‚ùå Error: {result['error']}\")\n",
        "        continue\n",
        "        \n",
        "    print(f\"\\nüìä Search Results:\\n{result['answer']}\")\n",
        "    \n",
        "    # Log to Quotient for quality monitoring\n",
        "    log_id = quotient.log(\n",
        "        user_query=query,\n",
        "        model_output=result['answer'],\n",
        "        documents=[str(doc) for doc in result['sources']]\n",
        "    )\n",
        "    \n",
        "    print(f\"üìù Logged to Quotient with log_id: {log_id}\")\n",
        "    log_ids.append(log_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### How It Works\n",
        "\n",
        "When `.log()` is called:\n",
        "\n",
        "1. **Data ingestion:** The query, model output, and all retrieved document contents are logged to Quotient.\n",
        "\n",
        "2. **Async detection pipeline:** Quotient runs:\n",
        "- **Hallucination detection**, labeling the output as hallucinated or not.\n",
        "- **Document relevance scoring**, marking which retrieved documents helped ground the output \n",
        "\n",
        "3. **Result retrieval:** You can poll or fetch detections linked to your `log_id`.\n",
        "\n",
        "4. **Monitor and troubleshoot in the Quotient app:** Access the [Quotient dashboard](app.quotientai.co) to:\n",
        "- Monitor your AI system over time\n",
        "- Review flagged hallucinated sentences\n",
        "- See which documents were irrelevant\n",
        "- Compare across tags or environments for deeper insights\n",
        "\n",
        "For full implementation details, visit the Quotient [docs](https://docs.quotientai.co/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Step 4: Review `Detections` and `Reports` in Quotient\n",
        "\n",
        "You can view your logs and detections in the [Quotient dashboard](app.quotientai.co), where you can filter them by tags and environments to identify common failure patterns.\n",
        "\n",
        "![Quotient AI Dashboard](Quotient_Dashboard.png \"Quotient AI Dashboard\")\n",
        "\n",
        "## üö® New: Daily Reports and Query Analysis\n",
        "\n",
        "Quotient now provides automated daily reports that help you understand your search agent's performance at scale. \n",
        "\n",
        "![Quotient Reports](Quotient_Reports.png \"Quotient AI Reports\")\n",
        "\n",
        "The new `Reports` tab offers:\n",
        "\n",
        "### 1. Semantic Query Clustering\n",
        "- Automatically groups similar queries together\n",
        "- Shows you what topics your users are most interested in\n",
        "- Helps identify patterns in search behavior\n",
        "\n",
        "### 2. Topic-Level Analytics\n",
        "- Hallucination rates and Document Relevance scores per topic cluster\n",
        "- Identifies which topics need the most attention\n",
        "\n",
        "### 3. Risk Assessment\n",
        "- Ranks issues by both volume and severity\n",
        "- Highlights high-risk areas that need immediate attention\n",
        "- Tracks performance trends over time\n",
        "  \n",
        "![Quotient Reports](Quotient_Reports_2.png \"Quotient AI Reports\")\n",
        "\n",
        "### Using Reports for Agent Improvement\n",
        "\n",
        "The Reports feature is particularly valuable for:\n",
        "1. **Content Gap Analysis**: Identify topics where your search agent consistently struggles\n",
        "2. **Query Optimization**: Find patterns in queries that lead to poor results\n",
        "3. **Resource Allocation**: Focus improvements on high-volume or high-risk areas\n",
        "4. **Trend Monitoring**: Track how system performance changes over time\n",
        "\n",
        "To access these insights, visit the `Reports` tab in your Quotient dashboard after running 100+ queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## What You've Built\n",
        "\n",
        "A production-ready search agent that:\n",
        "- Takes natural language queries as input\n",
        "- Returns comprehensive, well-researched answers\n",
        "- Provides source attribution for fact verification\n",
        "- Monitors accuracy through Quotient's hallucination detection\n",
        "- Verifies information quality with document relevance scoring\n",
        "\n",
        "You can scale this to monitor production traffic, benchmark retrieval and search performance, or compare different models side by side.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Understanding Result Quality\n",
        "\n",
        "When evaluating the company descriptions, pay attention to these metrics:\n",
        "\n",
        "- **Hallucination Rate**: Should be **< 5%**\n",
        "  - Higher rates might indicate:\n",
        "    - Outdated or conflicting company information online\n",
        "    - Need for more specific company identifiers in queries\n",
        "    - Issues with source document quality\n",
        "\n",
        "- **Document Relevance**: Should be **> 75%**\n",
        "  - Lower scores might suggest:\n",
        "    - Company name ambiguity (e.g., common names or multiple companies)\n",
        "    - Need for additional company identifiers (e.g., industry, location)\n",
        "    - Source documents discussing different companies or topics\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tavily-quotient-eval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
